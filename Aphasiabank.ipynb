{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oP9JaPl0T4H6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# change the OS to use your project folder as the working directory\n",
        "project_folder = \"/content/drive/MyDrive/Colab_Notebooks/Aphasia/\"\n",
        "os.chdir(project_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0myFaiktY0b"
      },
      "outputs": [],
      "source": [
        "def read_text_file(file_path):\n",
        " os.chdir(file_path)\n",
        " final_file=[]\n",
        " last_file=[]\n",
        " # iterate through all file\n",
        " for file in os.listdir(file_path):\n",
        "    if os.path.isdir(file)==True:\n",
        "      file_path_new=file_path+\"/\"+file\n",
        "      for file in os.listdir(file_path_new):\n",
        "        if file.startswith(\".\"):\n",
        "            continue\n",
        "        if file.endswith(\".xml\"):\n",
        "            document = f\"{file_path_new}\"+\"/\"+file\n",
        "        # call read text file function\n",
        "            with open(document, 'r', errors=\"ignore\") as f:\n",
        "               final_file.append(XMLreader_new(f))\n",
        "    # Check whether file is in text format or not\n",
        "    else:\n",
        "        if file.startswith(\".\"):\n",
        "            continue\n",
        "        if file.endswith(\".xml\"):\n",
        "            document = f\"{file_path}\"+\"/\"+file\n",
        "        # call read text file function\n",
        "            with open(document, 'r',  errors=\"ignore\") as f:\n",
        "               final_file.append(XMLreader_new(f))\n",
        " return final_file\n",
        "\n",
        "def XMLreader(filename=''):\n",
        "  tree = parse(filename)\n",
        "  root = tree.getroot()\n",
        "  file=[]\n",
        "  sentence=[]\n",
        "  for elem in root:\n",
        "      for gchild in elem:\n",
        "                if gchild.text==None:\n",
        "                    if sentence!=[]:\n",
        "                      file.append(sentence)\n",
        "                      sentence=[]\n",
        "                      continue\n",
        "                else:\n",
        "                    sentence.append(gchild.text)\n",
        "  return file\n",
        "\n",
        "def XMLreader_comp(filename=''):\n",
        "  corpus_relation=[]\n",
        "  corpus_word=[]\n",
        "  sentence_relation=[]\n",
        "  sentence_word=[]\n",
        "  from lxml import etree\n",
        "  #xmldoc = minidom.parse(filename)\n",
        "  #itemlist = xmldoc.getElementsByTagName('relation')\n",
        "  #print(memoryElem.text)\n",
        "  tree = parse(filename)\n",
        "  root = tree.getroot()\n",
        "  for elem in tree.findall('.//{http://www.talkbank.org/ns/talkbank}w'):\n",
        "        print(elem.text)\n",
        "        gra=elem.find('.//{http://www.talkbank.org/ns/talkbank}gra')\n",
        "        if gra==None:\n",
        "            continue\n",
        "        elif gra.attrib['index'] == '1':\n",
        "            corpus_word.append(sentence_word)\n",
        "            corpus_relation.append(sentence_relation)\n",
        "            sentence_word=[]\n",
        "            sentence_relation=[]\n",
        "            sentence_word.append(elem.text)\n",
        "            sentence_relation.append(gra.attrib['relation'])\n",
        "        else:\n",
        "            sentence_word.append(elem.text)\n",
        "            sentence_relation.append(gra.attrib['relation'])\n",
        "  return corpus_word\n",
        "def XMLreader_new(filename=''):\n",
        "  corpus_relation=[]\n",
        "  corpus_word=[]\n",
        "  sentence_relation=[]\n",
        "  sentence_word=[]\n",
        "  from lxml import etree\n",
        "  #xmldoc = minidom.parse(filename)\n",
        "  #itemlist = xmldoc.getElementsByTagName('relation')\n",
        "  #print(memoryElem.text)\n",
        "  tree = parse(filename)\n",
        "  root = tree.getroot()\n",
        "  for u in root.findall(\".//{http://www.talkbank.org/ns/talkbank}u\"):\n",
        "        sentence = []\n",
        "        for w in u.findall(\".//{http://www.talkbank.org/ns/talkbank}w\"):\n",
        "            sentence.append(w.text)\n",
        "        corpus_word.append(sentence)\n",
        "  return corpus_word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "T9jVb7xltVaw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from xml.etree.ElementTree import parse\n",
        "control_data=read_text_file(\"/content/drive/MyDrive/Colab_Notebooks/Aphasia/control\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2mpgemdi1uzs"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from xml.etree.ElementTree import parse\n",
        "aphasia_data=read_text_file(\"/content/drive/MyDrive/Colab_Notebooks/Aphasia/aphasia\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SkoUQKJWcMqo"
      },
      "outputs": [],
      "source": [
        "aphasia_label=[1]*len(aphasia_data)\n",
        "control_label=[0]*len(control_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WipclnBMaE_y"
      },
      "outputs": [],
      "source": [
        "all_data=aphasia_data+control_data\n",
        "all_label=aphasia_label+control_label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRP5HmuqZAId"
      },
      "source": [
        "### **chatgpt 4o**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOgIor9laFuu",
        "outputId": "69bbe0ee-e017-41bb-8994-dada96c18dc5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross-validation accuracies: [0.677570093457944, 0.6635514018691588, 0.6854460093896714, 0.6713615023474179, 0.6525821596244131]\n",
            "Mean accuracy: 0.670102233337721, Std: 0.011337129553777277\n",
            "F1 scores: [0.807799442896936, 0.797752808988764, 0.8133704735376044, 0.8033707865168539, 0.7897727272727273]\n",
            "Mean F1 score: 0.8024132478425772, Std: 0.00814237340085836\n",
            "Specificities: [0.677570093457944, 0.6635514018691588, 0.6854460093896714, 0.6713615023474179, 0.6525821596244131]\n",
            "Mean specificity: 0.670102233337721, Std: 0.011337129553777277\n",
            "Sensitivities: [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "Mean sensitivity: 1.0, Std: 0.0\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "import numpy as np\n",
        "\n",
        "def detect_repetitive_phrases(participant_lists):\n",
        "    for text_list in participant_lists:\n",
        "        text_counter = Counter(text_list)\n",
        "        if any(count > 1 for count in text_counter.values()):\n",
        "            return 1  # Label as aphasia\n",
        "    return 0  # Label as non-aphasia\n",
        "\n",
        "def process_participants(data):\n",
        "    labels = []\n",
        "    for participant_lists in data:\n",
        "        label = detect_repetitive_phrases(participant_lists)\n",
        "        labels.append(label)\n",
        "    return labels\n",
        "\n",
        "# Assuming all_data and all_label are provided\n",
        "# all_data: list of lists of lists (each participant's data)\n",
        "# all_label: list of integers (each participant's true label)\n",
        "# Example:\n",
        "# all_data = [\n",
        "#     [['www', 'www'], ['it', 'is', 'a', 'lovely', 'day', 'it', 'is', 'a', 'lovely', 'day'], ['the', 'sun', 'is', 'shining'], ['I', 'want', 'to', 'go', 'to', 'the', 'park', 'to', 'the', 'park']],\n",
        "#     [['hello', 'hello'], ['today', 'is', 'a', 'good', 'day', 'today', 'is', 'a', 'good', 'day'], ['the', 'weather', 'is', 'nice'], ['let', 'us', 'go', 'to', 'the', 'beach', 'to', 'the', 'beach']],\n",
        "#     # Add more participants here\n",
        "# ]\n",
        "# all_label = [1, 1, 0, ...]\n",
        "\n",
        "# Perform 5-fold cross-validation\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
        "data = all_data  # Use lists directly\n",
        "labels = all_label  # Use lists directly\n",
        "\n",
        "accuracies = []\n",
        "f1_scores = []\n",
        "specificities = []\n",
        "sensitivities = []\n",
        "\n",
        "for train_index, test_index in kf.split(data):\n",
        "    train_data = [data[i] for i in train_index]\n",
        "    test_data = [data[i] for i in test_index]\n",
        "    train_labels = [labels[i] for i in train_index]\n",
        "    test_labels = [labels[i] for i in test_index]\n",
        "\n",
        "    # Predict labels for test data\n",
        "    predicted_labels = process_participants(test_data)\n",
        "\n",
        "    accuracy = accuracy_score(test_labels, predicted_labels)\n",
        "    f1 = f1_score(test_labels, predicted_labels)\n",
        "    specificity = precision_score(test_labels, predicted_labels)\n",
        "    sensitivity = recall_score(test_labels, predicted_labels)\n",
        "\n",
        "    accuracies.append(accuracy)\n",
        "    f1_scores.append(f1)\n",
        "    specificities.append(specificity)\n",
        "    sensitivities.append(sensitivity)\n",
        "\n",
        "# Calculate standard deviations\n",
        "accuracy_std = np.std(accuracies)\n",
        "f1_std = np.std(f1_scores)\n",
        "specificity_std = np.std(specificities)\n",
        "sensitivity_std = np.std(sensitivities)\n",
        "\n",
        "# Print cross-validation results\n",
        "print(f\"Cross-validation accuracies: {accuracies}\")\n",
        "print(f\"Mean accuracy: {np.mean(accuracies)}, Std: {accuracy_std}\")\n",
        "print(f\"F1 scores: {f1_scores}\")\n",
        "print(f\"Mean F1 score: {np.mean(f1_scores)}, Std: {f1_std}\")\n",
        "print(f\"Specificities: {specificities}\")\n",
        "print(f\"Mean specificity: {np.mean(specificities)}, Std: {specificity_std}\")\n",
        "print(f\"Sensitivities: {sensitivities}\")\n",
        "print(f\"Mean sensitivity: {np.mean(sensitivities)}, Std: {sensitivity_std}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgxEEkRSQWnC",
        "outputId": "ca4b7f99-3fa5-4f04-db94-854ed04f456a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross-validation accuracies: [0.7429906542056075, 0.7663551401869159, 0.7605633802816901, 0.7323943661971831, 0.704225352112676]\n",
            "Mean accuracy: 0.7413057785968145, Std: 0.022160937903184418\n",
            "F1 scores: [0.8148148148148148, 0.8287671232876712, 0.8294314381270903, 0.8093645484949832, 0.7864406779661017]\n",
            "Mean F1 score: 0.8137637205381323, Std: 0.015732677062400474\n",
            "Specificities: [0.7960526315789473, 0.8066666666666666, 0.8104575163398693, 0.7756410256410257, 0.7435897435897436]\n",
            "Mean specificity: 0.7864815167632505, Std: 0.02462159834322401\n",
            "Sensitivities: [0.8344827586206897, 0.852112676056338, 0.8493150684931506, 0.8461538461538461, 0.8345323741007195]\n",
            "Mean sensitivity: 0.8433193446849488, Std: 0.007437766929497693\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "import numpy as np\n",
        "\n",
        "def generate_rating(text_list):\n",
        "    if text_list is None or len(text_list) == 0:\n",
        "        return 0\n",
        "    unique_words = set(text_list)\n",
        "    if len(unique_words) == 1:\n",
        "        return 1\n",
        "    if len(unique_words) <= 3:\n",
        "        return 2\n",
        "    if all(word is not None and len(word) <= 3 for word in text_list):\n",
        "        return 3\n",
        "    if len(text_list) <= 5:\n",
        "        return 4\n",
        "    if len(text_list) <= 10:\n",
        "        return 5\n",
        "    if len(text_list) <= 15:\n",
        "        return 6\n",
        "    if len(text_list) <= 20:\n",
        "        return 7\n",
        "    if len(text_list) <= 25:\n",
        "        return 8\n",
        "    if len(text_list) <= 30:\n",
        "        return 9\n",
        "    return 10\n",
        "\n",
        "def detect_repetitive_phrases(participant_lists):\n",
        "    for text_list in participant_lists:\n",
        "        if text_list is None:\n",
        "            continue\n",
        "        text_counter = Counter(text_list)\n",
        "        if any(count > 1 for count in text_counter.values()):\n",
        "            return 1  # Label as aphasia\n",
        "    return 0  # Label as non-aphasia\n",
        "\n",
        "def extract_features(participant_lists):\n",
        "    num_repetitive_phrases = sum(detect_repetitive_phrases([text_list]) for text_list in participant_lists if text_list is not None)\n",
        "    average_rating = np.mean([generate_rating(text_list) for text_list in participant_lists if text_list is not None])\n",
        "    return [num_repetitive_phrases, average_rating]\n",
        "\n",
        "# Assuming all_data and all_label are provided\n",
        "# all_data: list of lists of lists (each participant's data)\n",
        "# all_label: list of integers (each participant's true label)\n",
        "# Example:\n",
        "# all_data = [\n",
        "#     [['www', 'www'], ['it', 'is', 'a', 'lovely', 'day', 'it', 'is', 'a', 'lovely', 'day'], ['the', 'sun', 'is', 'shining'], ['I', 'want', 'to', 'go', 'to', 'the', 'park', 'to', 'the', 'park']],\n",
        "#     [['hello', 'hello'], ['today', 'is', 'a', 'good', 'day', 'today', 'is', 'a', 'good', 'day'], ['the', 'weather', 'is', 'nice'], ['let', 'us', 'go', 'to', 'the', 'beach', 'to', 'the', 'beach']],\n",
        "#     # Add more participants here\n",
        "# ]\n",
        "# all_label = [1, 1, 0, ...]\n",
        "\n",
        "# Perform 5-fold cross-validation\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
        "data = all_data  # Use lists directly\n",
        "labels = np.array(all_label)  # Convert to NumPy array for indexing\n",
        "\n",
        "features = np.array([extract_features(participant_lists) for participant_lists in data])\n",
        "\n",
        "accuracies = []\n",
        "f1_scores = []\n",
        "specificities = []\n",
        "sensitivities = []\n",
        "\n",
        "for train_index, test_index in kf.split(features):\n",
        "    X_train, X_test = features[train_index], features[test_index]\n",
        "    y_train, y_test = labels[train_index], labels[test_index]\n",
        "\n",
        "    model = LogisticRegression(random_state=1)\n",
        "    model.fit(X_train, y_train)\n",
        "    predicted_labels = model.predict(X_test)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, predicted_labels)\n",
        "    f1 = f1_score(y_test, predicted_labels)\n",
        "    specificity = precision_score(y_test, predicted_labels)\n",
        "    sensitivity = recall_score(y_test, predicted_labels)\n",
        "\n",
        "    accuracies.append(accuracy)\n",
        "    f1_scores.append(f1)\n",
        "    specificities.append(specificity)\n",
        "    sensitivities.append(sensitivity)\n",
        "\n",
        "# Calculate standard deviations\n",
        "accuracy_std = np.std(accuracies)\n",
        "f1_std = np.std(f1_scores)\n",
        "specificity_std = np.std(specificities)\n",
        "sensitivity_std = np.std(sensitivities)\n",
        "\n",
        "# Print cross-validation results\n",
        "print(f\"Cross-validation accuracies: {accuracies}\")\n",
        "print(f\"Mean accuracy: {np.mean(accuracies)}, Std: {accuracy_std}\")\n",
        "print(f\"F1 scores: {f1_scores}\")\n",
        "print(f\"Mean F1 score: {np.mean(f1_scores)}, Std: {f1_std}\")\n",
        "print(f\"Specificities: {specificities}\")\n",
        "print(f\"Mean specificity: {np.mean(specificities)}, Std: {specificity_std}\")\n",
        "print(f\"Sensitivities: {sensitivities}\")\n",
        "print(f\"Mean sensitivity: {np.mean(sensitivities)}, Std: {sensitivity_std}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2WRuPrgYO7Q"
      },
      "source": [
        "WAB-AQ\n",
        "Western Aphasia Battery-Aphasia Quotient 中的fluency\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27irPJzsY8La"
      },
      "source": [
        "### **chatGPT 4**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9nq67sSZGK3",
        "outputId": "4c481f05-adc9-4400-caef-e0fba6f67574"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: Mean = 0.67, Std = 0.16\n",
            "F1 Score: Mean = 0.74, Std = 0.17\n",
            "Sensitivity: Mean = 0.80, Std = 0.25\n",
            "Specificity: Mean = 0.41, Std = 0.30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import f1_score, recall_score, confusion_matrix, precision_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "\n",
        "def create_model():\n",
        "    # Use MaxAbsScaler which is recommended for sparse data and change solver and increase max_iter\n",
        "    pipeline = make_pipeline(MaxAbsScaler(), LogisticRegression(solver='saga', max_iter=1000))\n",
        "    return pipeline\n",
        "\n",
        "# Define classifier pipeline\n",
        "classifier = create_model()\n",
        "\n",
        "# Setup 5-fold cross-validation\n",
        "kf = StratifiedKFold(n_splits=5)\n",
        "f1_scores = []\n",
        "sensitivities = []\n",
        "specificities = []\n",
        "\n",
        "for train_index, test_index in kf.split(features, all_label):\n",
        "    train_data, test_data = features[train_index], features[test_index]\n",
        "    train_labels, test_labels = np.array(all_label)[train_index], np.array(all_label)[test_index]\n",
        "\n",
        "    classifier.fit(train_data, train_labels)\n",
        "    predictions = classifier.predict(test_data)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(test_labels, predictions)\n",
        "    f1 = f1_score(test_labels, predictions)\n",
        "    sensitivity = recall_score(test_labels, predictions)\n",
        "    tn, fp, fn, tp = confusion_matrix(test_labels, predictions).ravel()\n",
        "    specificity = tn / (tn+fp) if (tn+fp) > 0 else 0\n",
        "\n",
        "    accuracies.append(accuracy)\n",
        "    f1_scores.append(f1)\n",
        "    sensitivities.append(sensitivity)\n",
        "    specificities.append(specificity)\n",
        "\n",
        "# Calculate mean and standard deviation of the metrics\n",
        "acc_mean, acc_std = np.mean(accuracies), np.std(accuracies)\n",
        "f1_mean, f1_std = np.mean(f1_scores), np.std(f1_scores)\n",
        "sensitivity_mean, sensitivity_std = np.mean(sensitivities), np.std(sensitivities)\n",
        "specificity_mean, specificity_std = np.mean(specificities), np.std(specificities)\n",
        "\n",
        "print(\"Accuracy: Mean = {:.2f}, Std = {:.2f}\".format(acc_mean, acc_std))\n",
        "print(\"F1 Score: Mean = {:.2f}, Std = {:.2f}\".format(f1_mean, f1_std))\n",
        "print(\"Sensitivity: Mean = {:.2f}, Std = {:.2f}\".format(sensitivity_mean, sensitivity_std))\n",
        "print(\"Specificity: Mean = {:.2f}, Std = {:.2f}\".format(specificity_mean, specificity_std))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kg_DJXCwTyIu",
        "outputId": "6214ac2d-7e55-4582-8764-e3e4b1369161"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: Mean = 0.67, Std = 0.16\n",
            "F1 Score: Mean = 0.74, Std = 0.17\n",
            "Sensitivity: Mean = 0.80, Std = 0.25\n",
            "Specificity: Mean = 0.41, Std = 0.30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, confusion_matrix, precision_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "\n",
        "# Function to preprocess and flatten text data, handling None values\n",
        "def preprocess_text_data(all_data):\n",
        "    return [\" \".join([\" \".join(filter(None, sentence)) for sentence in participant]) for participant in all_data]\n",
        "\n",
        "# Basic feature extraction using TF-IDF combined with simple linguistic features\n",
        "def extract_features(text_data):\n",
        "    tfidf_vectorizer = TfidfVectorizer()\n",
        "    tfidf_features = tfidf_vectorizer.fit_transform(text_data)\n",
        "\n",
        "    # Calculate simple linguistic features\n",
        "    additional_features = []\n",
        "    for text in text_data:\n",
        "        words = text.split()\n",
        "        sentence_lengths = [len(sentence.split()) for sentence in text.split('.')]\n",
        "        avg_sentence_length = np.mean(sentence_lengths) if sentence_lengths else 0\n",
        "        type_token_ratio = len(set(words)) / len(words) if words else 0\n",
        "        additional_features.append([avg_sentence_length, type_token_ratio])\n",
        "\n",
        "    additional_features = np.array(additional_features)\n",
        "    return np.hstack((tfidf_features.toarray(), additional_features))\n",
        "\n",
        "# Create a logistic regression model\n",
        "def create_model():\n",
        "    return make_pipeline(MaxAbsScaler(), LogisticRegression(solver='saga', max_iter=1000))\n",
        "\n",
        "processed_data = preprocess_text_data(all_data)\n",
        "features = extract_features(processed_data)\n",
        "\n",
        "classifier = create_model()\n",
        "\n",
        "# Setup 5-fold cross-validation\n",
        "kf = StratifiedKFold(n_splits=5)\n",
        "accuracies=[]\n",
        "f1_scores = []\n",
        "sensitivities = []\n",
        "specificities = []\n",
        "\n",
        "for train_index, test_index in kf.split(features, all_label):\n",
        "    train_features, test_features = features[train_index], features[test_index]\n",
        "    train_labels, test_labels = np.array(all_label)[train_index], np.array(all_label)[test_index]\n",
        "\n",
        "    classifier.fit(train_features, train_labels)\n",
        "    predictions = classifier.predict(test_features)\n",
        "\n",
        "    accuracy = accuracy_score(test_labels, predictions)\n",
        "    f1 = f1_score(test_labels, predictions)\n",
        "    sensitivity = recall_score(test_labels, predictions)\n",
        "    tn, fp, fn, tp = confusion_matrix(test_labels, predictions).ravel()\n",
        "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "\n",
        "    accuracies.append(accuracy)\n",
        "    f1_scores.append(f1)\n",
        "    sensitivities.append(sensitivity)\n",
        "    specificities.append(specificity)\n",
        "\n",
        "# Calculate mean and standard deviation of the metrics\n",
        "acc_mean, acc_std = np.mean(accuracies), np.std(accuracies)\n",
        "f1_mean, f1_std = np.mean(f1_scores), np.std(f1_scores)\n",
        "sensitivity_mean, sensitivity_std = np.mean(sensitivities), np.std(sensitivities)\n",
        "specificity_mean, specificity_std = np.mean(specificities), np.std(specificities)\n",
        "\n",
        "print(\"Accuracy: Mean = {:.2f}, Std = {:.2f}\".format(acc_mean, acc_std))\n",
        "print(\"F1 Score: Mean = {:.2f}, Std = {:.2f}\".format(f1_mean, f1_std))\n",
        "print(\"Sensitivity: Mean = {:.2f}, Std = {:.2f}\".format(sensitivity_mean, sensitivity_std))\n",
        "print(\"Specificity: Mean = {:.2f}, Std = {:.2f}\".format(specificity_mean, specificity_std))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQYqjIINxunL",
        "outputId": "a9479a0a-fe6f-4fe2-e06f-a77472ab1dba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: Mean = 0.67, Std = 0.00\n",
            "F1 Score: Mean = 0.80, Std = 0.00\n",
            "Sensitivity: Mean = 1.00, Std = 0.00\n",
            "Specificity: Mean = 0.00, Std = 0.00\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, confusion_matrix, precision_score\n",
        "\n",
        "# Example feature calculation: average sentence length and type-token ratio (lexical diversity)\n",
        "def calculate_features(text):\n",
        "    sentences = text.split('.')\n",
        "    avg_sentence_length = np.mean([len(sentence.split()) for sentence in sentences if sentence.strip() != ''])\n",
        "    words = text.split()\n",
        "    lexical_diversity = len(set(words)) / len(words) if words else 0\n",
        "    return avg_sentence_length, lexical_diversity\n",
        "\n",
        "# Function to preprocess and flatten text data, handling None values\n",
        "def preprocess_text_data(all_data):\n",
        "    return [\" \".join([\" \".join(filter(None, sentence)) for sentence in participant]) for participant in all_data]\n",
        "\n",
        "# Threshold-based prediction for aphasia\n",
        "def predict_aphasia(features, sentence_length_threshold, diversity_threshold):\n",
        "    avg_sentence_length, lexical_diversity = features\n",
        "    if avg_sentence_length < sentence_length_threshold or lexical_diversity < diversity_threshold:\n",
        "        return 1  # Indicates aphasia\n",
        "    else:\n",
        "        return 0  # No aphasia\n",
        "\n",
        "# Perform 5-fold cross-validation to calculate accuracy, F1 score, sensitivity, and specificity\n",
        "def perform_cross_validation(features, labels):\n",
        "    kf = StratifiedKFold(n_splits=5)\n",
        "    accuracy_scores = []\n",
        "    f1_scores = []\n",
        "    sensitivity_scores = []\n",
        "    specificity_scores = []\n",
        "\n",
        "    for train_index, test_index in kf.split(features, labels):\n",
        "        test_features = [features[i] for i in test_index]\n",
        "        test_labels = [labels[i] for i in test_index]\n",
        "\n",
        "        predictions = [predict_aphasia(f, 5, 0.5) for f in test_features]  # Using example thresholds\n",
        "\n",
        "        accuracy_scores.append(accuracy_score(test_labels, predictions))\n",
        "        f1_scores.append(f1_score(test_labels, predictions))\n",
        "        sensitivity_scores.append(recall_score(test_labels, predictions))\n",
        "        tn, fp, fn, tp = confusion_matrix(test_labels, predictions).ravel()\n",
        "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "        specificity_scores.append(specificity)\n",
        "\n",
        "    # Calculate mean and standard deviation of the metrics\n",
        "    print_evaluation(accuracy_scores, f1_scores, sensitivity_scores, specificity_scores)\n",
        "\n",
        "# Print evaluation metrics\n",
        "def print_evaluation(accuracy_scores, f1_scores, sensitivity_scores, specificity_scores):\n",
        "    print(\"Accuracy: Mean = {:.2f}, Std = {:.2f}\".format(np.mean(accuracy_scores), np.std(accuracy_scores)))\n",
        "    print(\"F1 Score: Mean = {:.2f}, Std = {:.2f}\".format(np.mean(f1_scores), np.std(f1_scores)))\n",
        "    print(\"Sensitivity: Mean = {:.2f}, Std = {:.2f}\".format(np.mean(sensitivity_scores), np.std(sensitivity_scores)))\n",
        "    print(\"Specificity: Mean = {:.2f}, Std = {:.2f}\".format(np.mean(specificity_scores), np.std(specificity_scores)))\n",
        "\n",
        "processed_data = preprocess_text_data(all_data)\n",
        "features = [calculate_features(text) for text in processed_data]\n",
        "\n",
        "perform_cross_validation(features, all_label)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxeYdx9dnPmX"
      },
      "source": [
        "### **claude3**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAicfDEwnRX3",
        "outputId": "9294646c-1c79-4680-fbda-ae324d347295"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy: 0.605 ± 0.034\n",
            "f1_score: 0.623 ± 0.036\n",
            "specificity: 0.844 ± 0.037\n",
            "sensitivity: 0.488 ± 0.033\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "\n",
        "def extract_features(participant_utterances):\n",
        "    features = {\n",
        "        'repeated_words_ratio': 0,\n",
        "        'repeated_phrases_ratio': 0,\n",
        "        'short_sentences_ratio': 0,\n",
        "        'nonsense_words_ratio': 0\n",
        "    }\n",
        "    total_words = 0\n",
        "\n",
        "    for utterance in participant_utterances:\n",
        "        # Filter out None values\n",
        "        utterance = [word for word in utterance if word is not None]\n",
        "        total_words += len(utterance)\n",
        "        repeated_words = len([word for word in set(utterance) if utterance.count(word) > 1])\n",
        "        repeated_phrases = len(detect_repeated_phrases(utterance))\n",
        "        short_sentence = 1 if len(utterance) <= 3 else 0\n",
        "        nonsense_words = len([word for word in utterance if len(str(word)) > 1 and str(word).lower() not in basic_english_words])\n",
        "\n",
        "        features['repeated_words_ratio'] += repeated_words\n",
        "        features['repeated_phrases_ratio'] += repeated_phrases\n",
        "        features['short_sentences_ratio'] += short_sentence\n",
        "        features['nonsense_words_ratio'] += nonsense_words\n",
        "\n",
        "    for key in features:\n",
        "        features[key] /= max(total_words, 1)\n",
        "\n",
        "    return list(features.values())\n",
        "\n",
        "def classify_aphasia(features, threshold=0.1):\n",
        "    return int(any(feature > threshold for feature in features))\n",
        "\n",
        "def detect_repeated_phrases(word_list):\n",
        "    phrases = []\n",
        "    word_list = [str(word) for word in word_list if word is not None]  # Convert to string and filter None\n",
        "    for i in range(len(word_list)):\n",
        "        for j in range(i+2, len(word_list)):\n",
        "            phrase = ' '.join(word_list[i:j])\n",
        "            if word_list.count(phrase) > 1:\n",
        "                phrases.append(phrase)\n",
        "    return list(set(phrases))\n",
        "\n",
        "def cross_validate(X, y, n_splits=5):\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "    f1_scores = []\n",
        "    specificities = []\n",
        "    sensitivities = []\n",
        "    accuracies=[]\n",
        "\n",
        "    for train_index, test_index in skf.split(X, y):\n",
        "        X_train, X_test = X[train_index], X[test_index]\n",
        "        y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "        # Train (in this case, just set the threshold)\n",
        "        threshold = np.mean(X_train.max(axis=1))\n",
        "\n",
        "        # Predict\n",
        "        y_pred = [classify_aphasia(features, threshold) for features in X_test]\n",
        "\n",
        "        # Calculate metrics\n",
        "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
        "        accuracies.append(accuracy_score(y_test, y_pred))\n",
        "        f1_scores.append(f1_score(y_test, y_pred))\n",
        "        specificities.append(tn / (tn + fp))\n",
        "        sensitivities.append(tp / (tp + fn))\n",
        "\n",
        "    return {\n",
        "        'accuracy':(np.mean(accuracies), np.std(accuracies)),\n",
        "        'f1_score': (np.mean(f1_scores), np.std(f1_scores)),\n",
        "        'specificity': (np.mean(specificities), np.std(specificities)),\n",
        "        'sensitivity': (np.mean(sensitivities), np.std(sensitivities))\n",
        "    }\n",
        "\n",
        "# Basic set of common English words (you should expand this)\n",
        "basic_english_words = set(['the', 'be', 'to', 'of', 'and', 'a', 'in', 'that', 'have', 'it',\n",
        "                           'for', 'not', 'on', 'with', 'he', 'as', 'you', 'do', 'at', 'this',\n",
        "                           'but', 'his', 'by', 'from', 'they', 'we', 'say', 'her', 'she', 'or',\n",
        "                           'an', 'will', 'my', 'one', 'all', 'would', 'there', 'their', 'what',\n",
        "                           'so', 'up', 'out', 'if', 'about', 'who', 'get', 'which', 'go', 'me',\n",
        "                           'is', 'day', 'lovely', 'www'])\n",
        "\n",
        "# Use your data\n",
        "# all_data = # Your all_data variable here\n",
        "# all_label = # Your all_label variable here\n",
        "\n",
        "# Extract features for each participant\n",
        "X = np.array([extract_features(participant) for participant in all_data])\n",
        "y = np.array(all_label)\n",
        "\n",
        "# Perform cross-validation\n",
        "results = cross_validate(X, y)\n",
        "\n",
        "# Print results\n",
        "for metric, (mean, std) in results.items():\n",
        "    print(f\"{metric}: {mean:.3f} ± {std:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omyM6immnSxo",
        "outputId": "095bcdf9-2bfa-4327-d771-f131de49233c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy: 0.608 ± 0.036\n",
            "f1_score: 0.627 ± 0.039\n",
            "specificity: 0.844 ± 0.037\n",
            "sensitivity: 0.492 ± 0.036\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "import random\n",
        "\n",
        "def generate_rating(utterance):\n",
        "    # This is a simplified rating generation based on the scales you provided\n",
        "    # You may need to refine this based on more specific criteria\n",
        "    if not utterance or all(len(word) < 2 for word in utterance):\n",
        "        return 0\n",
        "    elif len(utterance) < 3:\n",
        "        return random.choice([1, 2])\n",
        "    elif len(utterance) < 5:\n",
        "        return random.choice([3, 4, 5])\n",
        "    elif len(utterance) < 8:\n",
        "        return random.choice([6, 7])\n",
        "    else:\n",
        "        return random.choice([8, 9, 10])\n",
        "\n",
        "def extract_features(participant_utterances):\n",
        "    features = {\n",
        "        'repeated_words_ratio': 0,\n",
        "        'repeated_phrases_ratio': 0,\n",
        "        'short_sentences_ratio': 0,\n",
        "        'nonsense_words_ratio': 0,\n",
        "        'average_rating': 0\n",
        "    }\n",
        "    total_words = 0\n",
        "    total_utterances = len(participant_utterances)\n",
        "\n",
        "    for utterance in participant_utterances:\n",
        "        utterance = [word for word in utterance if word is not None]\n",
        "        total_words += len(utterance)\n",
        "        repeated_words = len([word for word in set(utterance) if utterance.count(word) > 1])\n",
        "        repeated_phrases = len(detect_repeated_phrases(utterance))\n",
        "        short_sentence = 1 if len(utterance) <= 3 else 0\n",
        "        nonsense_words = len([word for word in utterance if len(str(word)) > 1 and str(word).lower() not in basic_english_words])\n",
        "\n",
        "        features['repeated_words_ratio'] += repeated_words\n",
        "        features['repeated_phrases_ratio'] += repeated_phrases\n",
        "        features['short_sentences_ratio'] += short_sentence\n",
        "        features['nonsense_words_ratio'] += nonsense_words\n",
        "        features['average_rating'] += generate_rating(utterance)\n",
        "\n",
        "    for key in features:\n",
        "        if key == 'average_rating':\n",
        "            features[key] /= total_utterances\n",
        "        else:\n",
        "            features[key] /= max(total_words, 1)\n",
        "\n",
        "    return list(features.values())\n",
        "\n",
        "def classify_aphasia(features, threshold=0.1):\n",
        "    # Adjust the classification based on the new features\n",
        "    linguistic_features = features[:4]  # First 4 features are the original ones\n",
        "    rating = features[4]  # The 5th feature is the average rating\n",
        "\n",
        "    if any(feature > threshold for feature in linguistic_features) or rating < 5:\n",
        "        return 1  # Classified as having aphasia\n",
        "    return 0  # Classified as not having aphasia\n",
        "\n",
        "def detect_repeated_phrases(word_list):\n",
        "    phrases = []\n",
        "    word_list = [str(word) for word in word_list if word is not None]\n",
        "    for i in range(len(word_list)):\n",
        "        for j in range(i+2, len(word_list)):\n",
        "            phrase = ' '.join(word_list[i:j])\n",
        "            if word_list.count(phrase) > 1:\n",
        "                phrases.append(phrase)\n",
        "    return list(set(phrases))\n",
        "\n",
        "def cross_validate(X, y, n_splits=5):\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "    f1_scores = []\n",
        "    specificities = []\n",
        "    sensitivities = []\n",
        "    accuracies=[]\n",
        "\n",
        "    for train_index, test_index in skf.split(X, y):\n",
        "        X_train, X_test = X[train_index], X[test_index]\n",
        "        y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "        # Train (in this case, just set the threshold)\n",
        "        threshold = np.mean(X_train[:, :4].max(axis=1))  # Only use linguistic features for threshold\n",
        "\n",
        "        # Predict\n",
        "        y_pred = [classify_aphasia(features, threshold) for features in X_test]\n",
        "\n",
        "        # Calculate metrics\n",
        "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
        "        accuracies.append(accuracy_score(y_test, y_pred))\n",
        "        f1_scores.append(f1_score(y_test, y_pred))\n",
        "        specificities.append(tn / (tn + fp))\n",
        "        sensitivities.append(tp / (tp + fn))\n",
        "\n",
        "    return {\n",
        "        'accuracy':(np.mean(accuracies), np.std(accuracies)),\n",
        "        'f1_score': (np.mean(f1_scores), np.std(f1_scores)),\n",
        "        'specificity': (np.mean(specificities), np.std(specificities)),\n",
        "        'sensitivity': (np.mean(sensitivities), np.std(sensitivities))\n",
        "    }\n",
        "\n",
        "# Basic set of common English words (you should expand this)\n",
        "basic_english_words = set(['the', 'be', 'to', 'of', 'and', 'a', 'in', 'that', 'have', 'it',\n",
        "                           'for', 'not', 'on', 'with', 'he', 'as', 'you', 'do', 'at', 'this',\n",
        "                           'but', 'his', 'by', 'from', 'they', 'we', 'say', 'her', 'she', 'or',\n",
        "                           'an', 'will', 'my', 'one', 'all', 'would', 'there', 'their', 'what',\n",
        "                           'so', 'up', 'out', 'if', 'about', 'who', 'get', 'which', 'go', 'me',\n",
        "                           'is', 'day', 'lovely', 'www'])\n",
        "\n",
        "# Use your data\n",
        "# all_data = # Your all_data variable here\n",
        "# all_label = # Your all_label variable here\n",
        "\n",
        "# Extract features for each participant\n",
        "X = np.array([extract_features(participant) for participant in all_data])\n",
        "y = np.array(all_label)\n",
        "\n",
        "# Perform cross-validation\n",
        "results = cross_validate(X, y)\n",
        "\n",
        "# Print results\n",
        "for metric, (mean, std) in results.items():\n",
        "    print(f\"{metric}: {mean:.3f} ± {std:.3f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
