{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":251,"status":"ok","timestamp":1724119292730,"user":{"displayName":"Kai-Ying Lin","userId":"08927965935496307580"},"user_tz":600},"id":"WlLLc9S1h5Ts"},"outputs":[],"source":["import os\n","# change the OS to use your project folder as the working directory\n","project_folder = \"/content/drive/MyDrive/Colab_Notebooks/depression_detection/\"\n","os.chdir(project_folder)"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":5844,"status":"ok","timestamp":1724119303813,"user":{"displayName":"Kai-Ying Lin","userId":"08927965935496307580"},"user_tz":600},"id":"DWuT7dupjsa6"},"outputs":[],"source":["import pandas as pd\n","final_text=[]\n","for file in os.listdir(project_folder):\n","  if file.endswith(\"_TRANSCRIPT.csv\"):\n","    data_features = pd.read_csv(file,delimiter='\\t')\n","    data_values= data_features['value']\n","    value_list = data_values.tolist()\n","    final_text.append(value_list)"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":267,"status":"ok","timestamp":1724119307184,"user":{"displayName":"Kai-Ying Lin","userId":"08927965935496307580"},"user_tz":600},"id":"r7m8TM3lhrMX"},"outputs":[],"source":["import pandas as pd\n","\n","train_labels = pd.read_csv('train_split_Depression_AVEC2017.csv')\n","dev_labels = pd.read_csv('dev_split_Depression_AVEC2017.csv')\n","test_labels = pd.read_csv('full_test_split.csv')\n"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1724119307506,"user":{"displayName":"Kai-Ying Lin","userId":"08927965935496307580"},"user_tz":600},"id":"zyYXyub7oIl9"},"outputs":[],"source":["test_labels=test_labels.rename(columns={\"PHQ_Binary\": \"PHQ8_Binary\"})"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":303,"status":"ok","timestamp":1724119308318,"user":{"displayName":"Kai-Ying Lin","userId":"08927965935496307580"},"user_tz":600},"id":"cKi2Tv-Mmbem"},"outputs":[],"source":["# If not, please adjust the column names accordingly\n","combined_data = pd.concat([train_labels, dev_labels])\n","combined_data = pd.concat([combined_data, test_labels])"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1724119309663,"user":{"displayName":"Kai-Ying Lin","userId":"08927965935496307580"},"user_tz":600},"id":"8dQZIr_tu1wq"},"outputs":[],"source":["combined_data=combined_data.sort_values(by=['Participant_ID'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":261,"status":"ok","timestamp":1723156961886,"user":{"displayName":"Kai-Ying Lin","userId":"08927965935496307580"},"user_tz":600},"id":"bHUCZAvMJ9PS","outputId":"6f75b2a6-64a8-4136-d4c4-de993506c4c7"},"outputs":[{"data":{"text/plain":["240"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["len(final_text[0])"]},{"cell_type":"markdown","metadata":{"id":"lRTE74_q3YNS"},"source":["### **CHATGPT 4**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6279,"status":"ok","timestamp":1723156896501,"user":{"displayName":"Kai-Ying Lin","userId":"08927965935496307580"},"user_tz":600},"id":"rOu2fAhImQ_P","outputId":"068533c7-8c05-49e2-9021-563dccb59186"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import cross_val_score, cross_validate\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.pipeline import make_pipeline\n","from sklearn.metrics import make_scorer, recall_score, f1_score, confusion_matrix\n","\n","\n","# Extract the 'PHQ8_Binary' label\n","labels = combined_data['PHQ8_Binary'].tolist()\n","\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import cross_validate\n","from sklearn.pipeline import make_pipeline\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.metrics import make_scorer, recall_score, confusion_matrix\n","\n","# Custom scorer for specificity\n","def specificity_score(y_true, y_pred):\n","    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n","    return tn / (tn + fp)\n","\n","\n","# Flatten each inner list to a single string\n","flattened_text = [' '.join(map(str, doc)) for doc in final_text]\n","\n","\n","# Define the pipeline\n","pipeline = make_pipeline(\n","    CountVectorizer(),  # Convert text data to numeric features\n","    LogisticRegression(max_iter=1000)  # Apply a logistic regression model\n",")\n","\n","# Define scorers\n","scoring = {\n","    'accuracy':'accuracy',\n","    'f1_score': 'f1',  # Directly use 'f1' string shortcut for f1 score\n","    'sensitivity': make_scorer(recall_score),  # recall is the same as sensitivity\n","    'specificity': make_scorer(specificity_score)\n","}\n","\n","# Perform 5-fold cross-validation using the pipeline and text data\n","results = cross_validate(pipeline, flattened_text, labels, cv=5, scoring=scoring, return_train_score=False)\n","\n","# Calculate mean and standard deviation for F1, sensitivity, and specificity\n","acc_mean=np.mean(results['test_accuracy'])\n","acc_std=np.std(results['test_accuracy'])\n","f1_mean = np.mean(results['test_f1_score'])\n","f1_std = np.std(results['test_f1_score'])\n","sensitivity_mean = np.mean(results['test_sensitivity'])\n","sensitivity_std = np.std(results['test_sensitivity'])\n","specificity_mean = np.mean(results['test_specificity'])\n","specificity_std = np.std(results['test_specificity'])\n","\n","# Print results\n","print(\"Accuracy - Mean: {:.3f}, Standard Deviation: {:.3f}\".format(acc_mean, acc_std))\n","print(\"F1 Score - Mean: {:.3f}, Standard Deviation: {:.3f}\".format(f1_mean, f1_std))\n","print(\"Sensitivity - Mean: {:.3f}, Standard Deviation: {:.3f}\".format(sensitivity_mean, sensitivity_std))\n","print(\"Specificity - Mean: {:.3f}, Standard Deviation: {:.3f}\".format(specificity_mean, specificity_std))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13869,"status":"ok","timestamp":1721097405442,"user":{"displayName":"Kai-Ying Lin","userId":"08927965935496307580"},"user_tz":600},"id":"ROpkcOaoL-au","outputId":"9e61fcf4-7c05-4784-81af-e5a570f1cbe7"},"outputs":[],"source":["import numpy as np\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import classification_report, f1_score, recall_score, confusion_matrix\n","from nltk.tokenize import word_tokenize\n","import nltk\n","nltk.download('punkt')\n","\n","\n","# Symptom keywords and their associated weights\n","\n","# Mapping of checklist items to keywords\n","symptom_keywords = {\n","    \"sad\": 3, \"depressed\": 3, \"down\": 2,\n","    \"unhappy\": 2, \"blue\": 2, \"miserable\": 2,\n","    \"crying\": 2, \"tearful\": 2, \"weeping\": 2,\n","    \"discouraged\": 2, \"disheartened\": 2, \"hopeless\": 3,\n","    \"worthless\": 3, \"inadequate\": 3, \"inferior\": 2,\n","    \"guilty\": 2, \"ashamed\": 2, \"remorse\": 1,\n","    \"self-criticism\": 1, \"blaming\": 1,\n","    \"indecisive\": 1, \"hesitant\": 1, \"uncertain\": 1,\n","    \"disinterest in relationships\": 2, \"social withdrawal\": 2, \"isolation\": 2,\n","    \"lonely\": 2, \"isolated\": 2, \"alone\": 2,\n","    \"avoiding social interactions\": 2, \"withdrawal\": 2, \"seclusion\": 2,\n","    \"unmotivated\": 2, \"apathetic\": 2, \"listless\": 2,\n","    \"disinterest in work\": 2, \"bored\": 2, \"apathetic about activities\": 2,\n","    \"avoiding responsibilities\": 2, \"neglecting duties\": 2, \"shirking\": 1,\n","    \"anhedonia\": 3, \"joyless\": 3, \"pleasureless\": 3,\n","    \"fatigue\": 1, \"exhausted\": 1, \"worn out\": 1,\n","    \"insomnia\": 1, \"oversleeping\": 1, \"sleep disturbances\": 1,\n","    \"eating less\": 1, \"eating more\": 1, \"appetite changes\": 1,\n","    \"low libido\": 1, \"disinterest in sex\": 1, \"sexual apathy\": 1,\n","    \"health anxiety\": 1, \"hypochondria\": 1, \"preoccupied with health\": 1\n","}\n","\n","\n","# Function to analyze text and generate a rating based on symptom keywords\n","def generate_rating(text):\n","    words = word_tokenize(text.lower())\n","    return sum(symptom_keywords.get(word, 0) for word in words)\n","\n","# Flatten each inner list to a single string\n","flattened_text = [' '.join(map(str, doc)) for doc in final_text]\n","ratings = [generate_rating(text) for text in flattened_text]\n","\n","# Convert text data to numeric features using CountVectorizer within a pipeline\n","vectorizer = CountVectorizer()\n","X_text = vectorizer.fit_transform(flattened_text).toarray()\n","\n","# Append ratings to feature matrix\n","ratings_array = np.array(ratings).reshape(-1, 1)\n","X_combined = np.hstack((X_text, ratings_array))\n","\n","# Extract the 'PHQ8_Binary' label\n","labels = combined_data['PHQ8_Binary'].tolist()\n","\n","\n","# Define the pipeline with LogisticRegression\n","pipeline = make_pipeline(\n","    LogisticRegression(max_iter=1000)\n",")\n","\n","# Define custom scorers\n","def specificity_score(y_true, y_pred):\n","    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n","    return tn / (tn + fp)\n","\n","scoring = {\n","    'accuracy':'accuracy',\n","    'f1_score': 'f1',\n","    'sensitivity': make_scorer(recall_score),\n","    'specificity': make_scorer(specificity_score)\n","}\n","\n","# Perform 5-fold cross-validation using the pipeline and combined data\n","results = cross_validate(pipeline, X_combined, labels, cv=5, scoring=scoring, return_train_score=False,  error_score='raise')\n","\n","# Calculate mean and standard deviation for F1, sensitivity, and specificity\n","acc_mean=np.mean(results['test_accuracy'])\n","acc_std=np.std(results['test_accuracy'])\n","f1_mean = np.mean(results['test_f1_score'])\n","f1_std = np.std(results['test_f1_score'])\n","sensitivity_mean = np.mean(results['test_sensitivity'])\n","sensitivity_std = np.std(results['test_sensitivity'])\n","specificity_mean = np.mean(results['test_specificity'])\n","specificity_std = np.std(results['test_specificity'])\n","\n","# Print results\n","print(\"Accuracy - Mean: {:.3f}, Standard Deviation: {:.3f}\".format(acc_mean, acc_std))\n","print(\"F1 Score - Mean: {:.3f}, Standard Deviation: {:.3f}\".format(f1_mean, f1_std))\n","print(\"Sensitivity - Mean: {:.3f}, Standard Deviation: {:.3f}\".format(sensitivity_mean, sensitivity_std))\n","print(\"Specificity - Mean: {:.3f}, Standard Deviation: {:.3f}\".format(specificity_mean, specificity_std))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8648,"status":"ok","timestamp":1721786513979,"user":{"displayName":"Kai-Ying Lin","userId":"08927965935496307580"},"user_tz":600},"id":"LDf7Ya_UCa0P","outputId":"e8fb377b-e784-4266-fcb9-c4a1753091c3"},"outputs":[],"source":["import numpy as np\n","from nltk.tokenize import word_tokenize\n","import nltk\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, f1_score\n","\n","nltk.download('punkt')\n","\n","# Define symptom keywords with adjusted weights and expanded coverage\n","symptom_keywords = {\n","    \"sad\": 3, \"depressed\": 4, \"down\": 3, \"unhappy\": 2, \"blue\": 2, \"miserable\": 3,\n","    \"crying\": 3, \"tearful\": 2, \"weeping\": 2, \"discouraged\": 3, \"disheartened\": 2, \"hopeless\": 4,\n","    \"worthless\": 4, \"inadequate\": 3, \"inferior\": 2, \"guilty\": 2, \"ashamed\": 2, \"remorse\": 2,\n","    \"self-criticism\": 2, \"blaming\": 2, \"indecisive\": 1, \"hesitant\": 1, \"uncertain\": 1,\n","    \"disinterest in relationships\": 3, \"social withdrawal\": 3, \"isolation\": 3,\n","    \"lonely\": 3, \"isolated\": 2, \"alone\": 2, \"avoiding social interactions\": 3, \"withdrawal\": 2, \"seclusion\": 2,\n","    \"unmotivated\": 3, \"apathetic\": 3, \"listless\": 2, \"disinterest in work\": 2, \"bored\": 2, \"apathetic about activities\": 2,\n","    \"avoiding responsibilities\": 3, \"neglecting duties\": 2, \"shirking\": 1,\n","    \"anhedonia\": 4, \"joyless\": 3, \"pleasureless\": 4,\n","    \"fatigue\": 2, \"exhausted\": 2, \"worn out\": 2, \"insomnia\": 2, \"oversleeping\": 2, \"sleep disturbances\": 2,\n","    \"eating less\": 2, \"eating more\": 2, \"appetite changes\": 2,\n","    \"low libido\": 2, \"disinterest in sex\": 2, \"sexual apathy\": 2,\n","    \"health anxiety\": 1, \"hypochondria\": 1, \"preoccupied with health\": 1\n","}\n","\n","# Function to calculate scores based on the presence and frequency of keywords\n","def calculate_scores(texts):\n","    scores = []\n","    for text_list in texts:\n","        # Convert each item in the list to string (if it's a list), then join\n","        text = ' '.join(str(item) for item in text_list) if isinstance(text_list, list) else str(text_list)\n","        words = word_tokenize(text.lower())\n","        score = sum(words.count(k) * v for k, v in symptom_keywords.items())\n","        scores.append(score)\n","    return np.array(scores)\n","\n","\n","# Pre-defined labels based on manual thresholding from an expert (for demonstration)\n","labels = np.array(combined_data['PHQ8_Binary'].tolist())\n","\n","# Calculate scores and classify based on a defined threshold, e.g., 50\n","scores = calculate_scores(final_text)\n","classified_results = (scores >= 10).astype(int)  # Applying threshold to classify\n","\n","# 5-Fold Stratified Cross-Validation to evaluate the effectiveness\n","skf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n","accuracies, sensitivities, specificities, f1scores = [], [], [], []\n","\n","for train_index, test_index in skf.split(scores, labels):\n","    y_train, y_test = labels[train_index], labels[test_index]\n","    y_pred = classified_results[test_index]\n","\n","    # Calculate metrics\n","    acc = accuracy_score(y_test, y_pred)\n","    rec = recall_score(y_test, y_pred)\n","    f1 = f1_score(y_test, y_pred)\n","    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n","    spec = tn / (tn + fp) if (tn + fp) > 0 else 0\n","\n","    accuracies.append(acc)\n","    sensitivities.append(rec)\n","    specificities.append(spec)\n","    f1scores.append(f1)\n","\n","# Output average scores and their standard deviations\n","print(\"Accuracy - Mean: {:.3f}, Std: {:.3f}\".format(np.mean(accuracies), np.std(accuracies)))\n","print(\"Sensitivity - Mean: {:.3f}, Std: {:.3f}\".format(np.mean(sensitivities), np.std(sensitivities)))\n","print(\"Specificity - Mean: {:.3f}, Std: {:.3f}\".format(np.mean(specificities), np.std(specificities)))\n","print(\"F1 Score - Mean: {:.3f}, Std: {:.3f}\".format(np.mean(f1scores), np.std(f1scores)))\n"]},{"cell_type":"markdown","metadata":{"id":"C_Hf2UUrQ0XF"},"source":["## **chatgpt 4o**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":106118,"status":"ok","timestamp":1721097566751,"user":{"displayName":"Kai-Ying Lin","userId":"08927965935496307580"},"user_tz":600},"id":"RsyYXAYuQ3X1","outputId":"215d7373-788b-4bd5-a597-187286943d1d"},"outputs":[],"source":["import pandas as pd\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import f1_score, accuracy_score, recall_score, confusion_matrix, precision_score\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from imblearn.over_sampling import SMOTE\n","import numpy as np\n","import xgboost as xgb\n","\n","# Define function to calculate specificity for binary classification\n","def specificity_score(y_true, y_pred):\n","    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n","    return tn / (tn + fp) if (tn + fp) != 0 else 0\n","\n","# Example data\n","# final_text should be your processed text data\n","# labels should be your binary labels\n","\n","# Assuming final_text is a list of lists containing tokens and labels is a list of binary labels\n","\n","# Flatten each inner list to a single string\n","flattened_text = [' '.join(map(str, doc)) for doc in final_text]\n","\n","# Vectorize the text data with bigrams included\n","vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n","X = vectorizer.fit_transform(flattened_text)\n","y = np.array(labels)  # Ensure y is a numpy array\n","\n","kf = KFold(n_splits=5)\n","\n","f1_scores = []\n","sensitivities = []\n","specificities = []\n","precisions = []\n","accuracies=[]\n","\n","for train_index, test_index in kf.split(X):\n","    X_train, X_test = X[train_index], X[test_index]\n","    y_train, y_test = y[train_index], y[test_index]\n","\n","    # Handle class imbalance using SMOTE\n","    smote = SMOTE()\n","    X_train, y_train = smote.fit_resample(X_train, y_train)\n","\n","    model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n","    model.fit(X_train, y_train)\n","\n","    y_pred = model.predict(X_test)\n","\n","    acc=accuracy_score(y_test, y_pred)\n","    f1 = f1_score(y_test, y_pred)\n","    sensitivity = recall_score(y_test, y_pred)\n","    specificity = specificity_score(y_test, y_pred)\n","    precision = precision_score(y_test, y_pred)\n","\n","    accuracies.append(acc)\n","    f1_scores.append(f1)\n","    sensitivities.append(sensitivity)\n","    specificities.append(specificity)\n","    precisions.append(precision)\n","\n","acc_mean = np.mean(accuracies)\n","acc_std = np.std(accuracies)\n","f1_mean = np.mean(f1_scores)\n","f1_std = np.std(f1_scores)\n","sensitivity_mean = np.mean(sensitivities)\n","sensitivity_std = np.std(sensitivities)\n","specificity_mean = np.mean(specificities)\n","specificity_std = np.std(specificities)\n","precision_mean = np.mean(precisions)\n","precision_std = np.std(precisions)\n","\n","print(f\"Accuracy: Mean = {acc_mean}, Std Dev = {acc_std}\")\n","print(f\"F1 Score: Mean = {f1_mean}, Std Dev = {f1_std}\")\n","print(f\"Sensitivity: Mean = {sensitivity_mean}, Std Dev = {sensitivity_std}\")\n","print(f\"Specificity: Mean = {specificity_mean}, Std Dev = {specificity_std}\")\n","print(f\"Precision: Mean = {precision_mean}, Std Dev = {precision_std}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":112057,"status":"ok","timestamp":1721097859557,"user":{"displayName":"Kai-Ying Lin","userId":"08927965935496307580"},"user_tz":600},"id":"9jJ9YoR8w0kz","outputId":"f1a19fc7-79c8-4060-cef4-fc39d6e43873"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: Mean = 0.6781887150308203, Std Dev = 0.1253890805876333\n","F1 Score: Mean = 0.2129718234981393, Std Dev = 0.1587420366113971\n","Sensitivity: Mean = 0.22326340326340324, Std Dev = 0.23895984320099856\n","Specificity: Mean = 0.8999444068815265, Std Dev = 0.07337868398955903\n","Precision: Mean = 0.38571428571428573, Std Dev = 0.3402013542760334\n"]}],"source":["import pandas as pd\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import f1_score, recall_score, confusion_matrix, precision_score\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LinearRegression\n","from imblearn.over_sampling import SMOTE\n","import numpy as np\n","import xgboost as xgb\n","\n","# Define function to calculate specificity for binary classification\n","def specificity_score(y_true, y_pred):\n","    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n","    return tn / (tn + fp) if (tn + fp) != 0 else 0\n","\n","# Example data\n","# final_text should be your processed text data\n","# labels should be your binary labels\n","\n","# Assuming final_text is a list of lists containing tokens and labels is a list of binary labels\n","\n","# Flatten each inner list to a single string\n","flattened_text = [' '.join(map(str, doc)) for doc in final_text]\n","\n","# Vectorize the text data with bigrams included\n","vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n","X_text = vectorizer.fit_transform(flattened_text)\n","\n","# Define the symptoms from Burnâ€™s Depression Checklist\n","symptoms = [\n","    'Feeling sad or down in the dumps', 'Feeling unhappy or blue', 'Crying spells or tearfulness',\n","    'Feeling discouraged', 'Feeling hopeless', 'Low self-esteem', 'Feeling worthless or inadequate',\n","    'Guilt or shame', 'Criticizing yourself or blaming others', 'Difficulty making decisions',\n","    'Loss of interest in family, friends or colleagues', 'Loneliness', 'Spending less time with family or friends',\n","    'Loss of motivation', 'Loss of interest in work or other activities', 'Avoiding work or other activities',\n","    'Loss of pleasure or satisfaction in life', 'Feeling tired', 'Difficulty sleeping or sleeping too much',\n","    'Decreased or increased appetite', 'Loss of interest in sex', 'Worrying about your health',\n","    'Do you have any suicidal thoughts?', 'Would you like to end your life?', 'Do you have a plan for harming yourself?'\n","]\n","\n","# Placeholder for symptom ratings\n","symptom_ratings = np.zeros((X_text.shape[0], len(symptoms)))\n","\n","# Regression model to predict symptom ratings (for simplicity, using Linear Regression)\n","regressor = LinearRegression()\n","\n","# Train a regression model for each symptom and predict ratings\n","for i, symptom in enumerate(symptoms):\n","    # Use some heuristic to create a target variable for training (example: keyword matching)\n","    # Here we use a dummy target variable, in practice you need a properly labeled dataset\n","    y_symptom = np.random.randint(0, 5, size=X_text.shape[0])\n","\n","    regressor.fit(X_text, y_symptom)\n","    symptom_ratings[:, i] = regressor.predict(X_text)\n","\n","# Sum the predicted ratings to get the total score\n","total_scores = symptom_ratings.sum(axis=1)\n","\n","# Define the depression levels\n","depression_levels = {\n","    (0, 5): 'No Depression',\n","    (6, 10): 'Normal but unhappy',\n","    (11, 25): 'Mild depression',\n","    (26, 50): 'Moderate depression',\n","    (51, 75): 'Severe depression',\n","    (76, 100): 'Extreme depression'\n","}\n","\n","# Convert total scores to depression levels\n","def get_depression_level(score):\n","    for (low, high), level in depression_levels.items():\n","        if low <= score <= high:\n","            return level\n","    return 'Unknown'\n","\n","depression_labels = np.array([get_depression_level(score) for score in total_scores])\n","\n","# Convert depression levels to binary labels (example: Severe and Extreme depression as positive class)\n","binary_labels = np.array([1 if level in ['Severe depression', 'Extreme depression'] else 0 for level in depression_labels])\n","\n","# Extract the 'PHQ8_Binary' label\n","labels = combined_data['PHQ8_Binary'].tolist()\n","y = np.array(labels)  # Ensure y is a numpy array\n","\n","kf = KFold(n_splits=5)\n","\n","f1_scores = []\n","sensitivities = []\n","specificities = []\n","precisions = []\n","\n","for train_index, test_index in kf.split(X_text):\n","    X_train, X_test = X_text[train_index], X_text[test_index]\n","    y_train, y_test = y[train_index], y[test_index]\n","\n","    # Handle class imbalance using SMOTE\n","    smote = SMOTE()\n","    X_train, y_train = smote.fit_resample(X_train, y_train)\n","\n","    model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n","    model.fit(X_train, y_train)\n","\n","    y_pred = model.predict(X_test)\n","\n","    acc=accuracy_score(y_test, y_pred)\n","    f1 = f1_score(y_test, y_pred)\n","    sensitivity = recall_score(y_test, y_pred)\n","    specificity = specificity_score(y_test, y_pred)\n","    precision = precision_score(y_test, y_pred)\n","\n","    accuracies.append(acc)\n","    f1_scores.append(f1)\n","    sensitivities.append(sensitivity)\n","    specificities.append(specificity)\n","    precisions.append(precision)\n","\n","acc_mean = np.mean(accuracies)\n","acc_std = np.std(accuracies)\n","f1_mean = np.mean(f1_scores)\n","f1_std = np.std(f1_scores)\n","sensitivity_mean = np.mean(sensitivities)\n","sensitivity_std = np.std(sensitivities)\n","specificity_mean = np.mean(specificities)\n","specificity_std = np.std(specificities)\n","precision_mean = np.mean(precisions)\n","precision_std = np.std(precisions)\n","\n","print(f\"Accuracy: Mean = {acc_mean}, Std Dev = {acc_std}\")\n","print(f\"F1 Score: Mean = {f1_mean}, Std Dev = {f1_std}\")\n","print(f\"Sensitivity: Mean = {sensitivity_mean}, Std Dev = {sensitivity_std}\")\n","print(f\"Specificity: Mean = {specificity_mean}, Std Dev = {specificity_std}\")\n","print(f\"Precision: Mean = {precision_mean}, Std Dev = {precision_std}\")\n","\n"]},{"cell_type":"markdown","metadata":{"id":"KVQ1kh_NFfsu"},"source":["### **Claude 3**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1456,"status":"ok","timestamp":1722896702027,"user":{"displayName":"Kai-Ying Lin","userId":"08927965935496307580"},"user_tz":600},"id":"B-4kYTl6FixY","outputId":"3a257efc-570d-49b2-94e0-0f8285193202"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import confusion_matrix, f1_score\n","from sklearn.metrics import f1_score, recall_score, confusion_matrix, precision_score, accuracy_score\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from imblearn.over_sampling import SMOTE\n","\n","# Assuming final_text is your list of lists containing text data\n","# final_text = [[utterance1, utterance2, ...], [utterance1, utterance2, ...], ...]\n","\n","# Extract the 'PHQ8_Binary' label\n","labels = combined_data['PHQ8_Binary'].tolist()\n","\n","# Calculate specificity and sensitivity\n","def calculate_metrics(y_true, y_pred):\n","    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n","    specificity = tn / (tn + fp)\n","    sensitivity = tp / (tp + fn)\n","    f1 = f1_score(y_true, y_pred)\n","    return specificity, sensitivity, f1\n","\n","# Read the labels data\n","labels_df = pd.read_csv('train_split_Depression_AVEC2017.csv')\n","\n","# Preprocess transcript data\n","def preprocess_transcript(utterances):\n","    # Convert all elements to strings and join\n","    return ' '.join(str(utterance) for utterance in utterances if utterance is not None)\n","\n","# Extract features from transcripts\n","def extract_features(texts):\n","    vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n","    features = vectorizer.fit_transform(texts)\n","    return pd.DataFrame(features.toarray(), columns=vectorizer.get_feature_names_out())\n","\n","# Preprocess transcripts\n","processed_transcripts = [preprocess_transcript(transcript) for transcript in final_text]\n","\n","# Extract features\n","features = extract_features(processed_transcripts)\n","\n","# Prepare data for cross-validation\n","X = features\n","y = np.array(labels)  # Ensure y is a numpy array\n","\n","# Initialize StratifiedKFold\n","skf = StratifiedKFold(n_splits=5, shuffle=True)\n","\n","# Initialize lists to store performance metrics\n","specificities = []\n","sensitivities = []\n","f1_scores = []\n","accuracies=[]\n","\n","# Perform 5-fold cross-validation\n","for fold, (train_index, val_index) in enumerate(skf.split(X, y), 1):\n","    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n","    y_train, y_val = y[train_index], y[val_index]\n","\n","    # Scale the features\n","    scaler = StandardScaler()\n","    X_train_scaled = scaler.fit_transform(X_train)\n","    X_val_scaled = scaler.transform(X_val)\n","\n","    # Apply SMOTE for oversampling\n","    smote = SMOTE(random_state=42)\n","    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n","\n","    # Train the model\n","    model = LogisticRegression(max_iter=1000, class_weight='balanced')\n","    model.fit(X_train_resampled, y_train_resampled)\n","\n","    # Make predictions\n","    y_pred = model.predict(X_val_scaled)\n","\n","    # Calculate metrics\n","    specificity, sensitivity, f1 = calculate_metrics(y_val, y_pred)\n","    acc=accuracy_score(y_val, y_pred)\n","\n","    # Store metrics\n","    accuracies.append(acc)\n","    specificities.append(specificity)\n","    sensitivities.append(sensitivity)\n","    f1_scores.append(f1)\n","\n","    print(f\"Fold {fold} - Specificity: {specificity:.4f}, Sensitivity: {sensitivity:.4f}, F1-score: {f1:.4f}\")\n","\n","# Print average metrics\n","print(\"\\nAverage metrics:\")\n","\n","print(f\"Accuracy: {np.mean(accuracies):.4f} (+/- {np.std(accuracies):.4f})\")\n","print(f\"Specificity: {np.mean(specificities):.4f} (+/- {np.std(specificities):.4f})\")\n","print(f\"Sensitivity: {np.mean(sensitivities):.4f} (+/- {np.std(sensitivities):.4f})\")\n","print(f\"F1-score: {np.mean(f1_scores):.4f} (+/- {np.std(f1_scores):.4f})\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":374,"status":"ok","timestamp":1722896508937,"user":{"displayName":"Kai-Ying Lin","userId":"08927965935496307580"},"user_tz":600},"id":"0Fw6CgtOcpCp","outputId":"d592a5bc-0553-4839-b45f-7b7fb81bdef2"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n","\n","# Improved keyword list with weights\n","checklist_keywords = {\n","    'sad': (['sad', 'down', 'unhappy', 'blue', 'cry', 'tear'], 2),\n","    'discouraged': (['discouraged', 'hopeless'], 2),\n","    'self_esteem': (['worthless', 'inadequate', 'guilt', 'shame'], 2),\n","    'criticizing': (['criticize', 'blame'], 1),\n","    'decisions': (['difficult', 'decision'], 1),\n","    'loss_interest': (['loss of interest', 'lonely', 'less time'], 2),\n","    'motivation': (['unmotivated', 'avoid work'], 2),\n","    'pleasure': (['no pleasure', 'no satisfaction'], 2),\n","    'tired': (['tired', 'fatigue'], 1),\n","    'sleep': (['sleep problem', 'insomnia'], 1),\n","    'appetite': (['appetite', 'eating'], 1),\n","    'sex': (['sex'], 1),\n","    'health': (['health', 'worry'], 1),\n","    'suicidal': (['suicide', 'end life', 'harm myself'], 3)\n","}\n","\n","def estimate_weighted_ratings(text):\n","    ratings = []\n","    for category, (keywords, weight) in checklist_keywords.items():\n","        count = sum(text.lower().count(keyword) for keyword in keywords)\n","        rating = min(count * weight, 4)  # Cap at 4\n","        ratings.append(rating)\n","    return ratings\n","\n","def preprocess_transcript(text_list):\n","    return ' '.join(str(item) for item in text_list).lower()\n","\n","# Preprocess transcripts and estimate ratings\n","processed_transcripts = []\n","estimated_ratings = []\n","for transcript in final_text:\n","    processed_text = preprocess_transcript(transcript)\n","    processed_transcripts.append(processed_text)\n","    estimated_ratings.append(estimate_weighted_ratings(processed_text))\n","\n","# Create a DataFrame with estimated ratings\n","ratings_df = pd.DataFrame(estimated_ratings, columns=checklist_keywords.keys())\n","\n","# Calculate total score\n","ratings_df['total_score'] = ratings_df.sum(axis=1)\n","\n","# Prepare data for cross-validation\n","X = ratings_df['total_score'].values\n","# Extract the 'PHQ8_Binary' label\n","labels = np.array(combined_data['PHQ8_Binary'].tolist())\n","\n","# Initialize StratifiedKFold\n","skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","\n","# Initialize lists to store performance metrics\n","after_accuracies = []\n","after_f1_scores = []\n","after_specificities = []\n","after_sensitivities = []\n","\n","# Perform 5-fold cross-validation\n","for fold, (train_index, val_index) in enumerate(skf.split(X, y), 1):\n","    X_val = X[val_index]\n","    y_true = y[val_index]\n","\n","    # Apply threshold to get predictions (using 50 as the threshold)\n","    y_pred = (X_val > 10).astype(int)\n","\n","    # Calculate metrics\n","    accuracy = accuracy_score(y_true, y_pred)\n","    f1 = f1_score(y_true, y_pred)\n","    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n","    specificity = tn / (tn + fp)\n","    sensitivity = tp / (tp + fn)\n","\n","    # Store metrics\n","    after_accuracies.append(accuracy)\n","    after_f1_scores.append(f1)\n","    after_specificities.append(specificity)\n","    after_sensitivities.append(sensitivity)\n","\n","    print(f\"Fold {fold}:\")\n","    print(f\"  Accuracy: {after_accuracies:}\")\n","    print(f\"  F1-score: {after_f1_scores:}\")\n","    print(f\"  Specificity: {after_specificities:}\")\n","    print(f\"  Sensitivity: {after_sensitivities:}\")\n","\n","# Print average metrics with standard deviations\n","print(\"\\nAverage metrics:\")\n","print(f\"Accuracy: {np.mean(after_accuracies):.4f} (+/- {np.std(after_accuracies):.4f})\")\n","print(f\"F1-score: {np.mean(after_f1_scores):.4f} (+/- {np.std(after_f1_scores):.4f})\")\n","print(f\"Specificity: {np.mean(after_specificities):.4f} (+/- {np.std(after_specificities):.4f})\")\n","print(f\"Sensitivity: {np.mean(after_sensitivities):.4f} (+/- {np.std(after_sensitivities):.4f})\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vxfl9QP_pPSA"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
